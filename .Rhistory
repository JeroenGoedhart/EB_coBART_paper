## MCMC Convergence check
if (i==1){
if (Info==T){
print("Check convergence of mcmc chains")
}
samps=fit$yhat.train[,sample(1:length(Y),1)]
samps<-matrix(samps, nrow=ndpost,ncol = nchain, byrow = T)
Rhat <- .MCMC_convergence(samps)
if (Info==T){
print(paste("Rhat equals: ",Rhat))
}
remove(samps)
if(Rhat<1.1){
if (Info==T){
print("convergence okay")
}
} else {
stop("Not converged yet, please change mcmc sampling settings")
}
}
## Estimate WAIC
Ypred = pnorm(fit$yhat.train)
Ypred[which(Ypred==0)] <- .0000000000000001
Ypred[which(Ypred==1)] <- .9999999999999999
LogLikMatrix = .LikelihoodBin(Ypred = Ypred, Y = Y)
WAICVector[i] <- suppressWarnings(loo::waic(LogLikMatrix)$estimates[3,1])
if (Info==T){
print(paste("WAIC equals: ",WAICVector[i]))
}
}
#### convergence check of EM algorithm
if (WAICVector[i]>WAIC_Old) {
EstProb = EstimatedProbs[i-1,]
EstWAIC = WAIC_Old
CodataModel <-  Codatamodels[[i-1]]
if (EB == T) {
Estk = k_Update[i-1]
EstAlpha = alpha_Update[i-1]
}
break
} else {
WAIC_Old <- WAICVector[i]
}
# obtain average number of times each variable occurs in the splitting rules
VarsUsed <- colSums(fit$varcount)  # count of each variable occuring in the splitting rules
VarsUsed <- VarsUsed/sum(VarsUsed) # normalize count of each variable to probabilities = pure EB updates of hyperparameter S
### STEP 2: Fit co-data model ###
coDataModel <- glm(VarsUsed ~.-1,
data=CoData,family=quasibinomial) # the model
Codatamodels[[i]] <- coDataModel
probs <- predict(coDataModel, type="response", newdata = CoData) # estimating the co-data moderated estimates of hyperparameter S
probs[is.na(probs)] <- 0
probs <-unname(probs)
EstimatedProbs[i+1,] <- probs
## Optional step: update other hyperparameters (alpha and k) of BART using Empirical Bayes ##
if (EB == T) {
trees <- dbarts::extract(fit,"trees",chainNum = c(1:nchain), sampleNum=c(sample(1:ndpost,0.1*ndpost,replace = F))) # tree structures, for computation, we only select a part of the tree
#trees <- extract(fit,"trees") # tree structures, for computation, we only select a part of the tree
# Update leaf node parameter k
k <- .EstimateLeafNode(Trees = trees, ntree = ntree, model = model)[2]
k_Update[i] <- k
# Update tree structure parameter alpha, we keep beta fixed
trees <- trees[c("tree", "sample", "chain", "n","var","value" )]
trees$depth <- unname(unlist(by(trees, trees[,c("tree", "sample", "chain")], .getDepth)))
alpha <- optim(alpha,.LikelihoodTreeStructure, beta = beta, Trees=trees, method = 'Brent', lower = .00001, upper = .9999999)$par
alpha_Update[i] <- alpha
remove(trees)
}
if (i==nIter){
print("EM algorithm not converged yet, consider increasing nIter")
print("Return estimates at last iteration")
EstProb = EstimatedProbs[i,]
EstWAIC = WAICVector[i]
CodataModel <-  Codatamodels[[i]]
if (EB == T) {
Estk = k_Update[1]
EstAlpha = alpha_Update[i]
}
}
}
# collect results
if (EB == T){
res <- list(SplittingProbs = EstProb, Codatamodel= CodataModel, k_ests = Estk,alpha_ests = EstAlpha)
} else {
res <- list(SplittingProbs = EstProb, Codatamodels= CodataModel)
}
return(res)
}
###################################
####### Auxiliary Functions #######
###################################
.FiniteSum <- function(x) {
sum(x[is.finite(x)])
}
.MCMC_convergence <- function(Samples){
if (!suppressMessages(require(posterior, quietly = T))) {stop("Package posterior not installed")}
if (class(Samples)[1] != "matrix") {stop("Samples should be specified as matrix with nsample rows and nchain columns")}
if(nrow(Samples)<ncol(Samples)){print("Are you sure Samples is specified by nsample rows and nchain columns")}
Rhat = posterior::rhat(Samples)
return(Rhat)
}
.LikelihoodBin <- function(Ypred,Y){
result <- apply(Ypred,1, function(x) Y*log(x)+(1-Y)*log(1-x))
return(t(result))
}
.LikelihoodCont <- function(Ypred, Y,sigma){
# returns the normal likelihoods for the sampled posterior parameters
# output is a N x m matrix, with N the number of samples and m the number of mc samples
loglik <- -(0.5*(1/sigma^2))*(sweep(Ypred,2,Y)^2)-.5*log(sigma^2)-.5*log(2*pi)
return(loglik)
}
.getDepth <- function(tree) {
getDepthRecurse <- function(tree, depth) {
node <- list(
depth = depth
)
if (tree$var[1] == -1) {
node$n_nodes <- 1
return(node)
}
headOfLeftBranch <- tree[-1,]
left <- getDepthRecurse(headOfLeftBranch, depth + 1)
n_nodes.left <- left$n_nodes
left$n_nodes <- NULL
headOfRightBranch <- tree[seq.int(2 + n_nodes.left, nrow(tree)),]
right <- getDepthRecurse(headOfRightBranch, depth + 1)
n_nodes.right <- right$n_nodes
right$n_nodes <- NULL
node$n_nodes <- 1 + n_nodes.left + n_nodes.right
node$depth <- c(node$depth, left$depth, right$depth)
return(node)
}
result <- getDepthRecurse(tree, 0)
return(result$depth)
}
.LikelihoodTreeStructure <- function(alpha,beta, Trees) {
LogLike <- ifelse(Trees$var==-1,log(1-alpha*(1+Trees$depth)^(-beta)),log(alpha*(1+Trees$depth)^(-beta)))
S <- .FiniteSum(LogLike)
return(-S)
}
.EstimateLeafNode <- function(Trees, ntree, model) {
if(!(model == "continuous" || model =="binary")){stop("model should be specified as continuous or binary")}
ids <- which(Trees$var==-1) #check which rows correspond to leaf nodes
samples <- Trees$value[ids] #obtain samples of leaf nodes
varhat <- (1/length(samples))*.FiniteSum(samples^2) #maximum likelihood estimate of variance for known mean (equals 0)
if (model=="continuous"){cnst = 0.5}
if (model=="binary"){cnst = 3}
k_hat <- cnst/(sqrt(varhat)*sqrt(ntree))
return(c(varhat=varhat,k_hat=k_hat))
}
a=EBcoBART(Y=Y,X=X,CoData = CoDat, nIter = 15, model = "binary",
EB = T, Info = T, Seed = T,
k = 2, alpha = .95, beta = 2)
a$SplittingProbs
a$k_ests
a$alpha_ests
Dat_EBcoBART <- function(X,CoData){
## ---------------------------------------------------------------------
## Convenience function to recreate CoData matrix if factor variables
## in X are present. dbarts uses dummy encoding for factor variables so
## CoData matrix should have co-data for each dummy. This function does
## that for you.
## ---------------------------------------------------------------------
## Arguments ##
# X: covariate model matrix, should be a data.frame.
#
# CoData: The codata model matrix with co-data information on the p covariates.
# If grouping information is present, please encode this yourself using dummies
# The number of rows of the co-data matrix should equal the number of columns of X
if (ncol(X) == 0 | nrow(X) == 0){stop("X not specified")}
if (ncol(CoData) == 0 | nrow(CoData) == 0){stop("CoData  not specified")}
if(class(CoData)[1] != "matrix" ){stop("CoData should be specified as matrix)")}
if (class(X) != "data.frame"){stop("X should be specified as data.frame)")}
if(ncol(X) != nrow(CoData)){stop("number of columns of X should equal number of rows of CoData")}
idVars <- which(sapply(X, is.factor))
replication_times <- rep(1,nrow(CoData))
if (length(idVars) > 0){
for (i in 1:length(idVars)) {
id <- idVars[i]
reps <- length(unique(X[,id]))
replication_times[id] <- reps
remove(id,reps)
}
}
CoDat <- CoData[rep(seq(1:nrow(CoData)), times = replication_times), ]
X <- model.matrix(~ . + 0, X)
res <- list(X = X, CoData = CoDat)
return(res)
}
EBcoBART <- function(Y,X,CoData, model,
nIter = 10, EB = F,
Prob_Init = c(rep(1/ncol(X),ncol(X))),
Info = F, Seed = T,
ndpost = 5000, # number of posterior samples after burn-in period
nskip = 5000, # number of burn-in samples
nchain = 5, # number of independent mcmc chains
keepevery = 1, # optional thinning
ntree = 50, # number of trees in the BART model
alpha = .95, beta = 2, k = 2, #Tree hyperparameters
sigest = sd(Y)*0.667, sigdf = 10, sigquant = .75 #Error variance hyperparameters, only required for continuous outcome
){
## ---------------------------------------------------------------------
## Co-data guided Empirical Bayes estimates of splitting probabilities of
## BART model
## ---------------------------------------------------------------------
## Arguments ##
# Y: response should be a numeric. For a binary model, specify Y as numeric with only zero's and ones
# X: covariate model matrix, should be a matrix object. If X contains factor variables,
# please consider Dat_EBcoBART function to set X and CoData matrix in the right format
# dbarts uses dummy encoding for factors(with dummies for each category and no intercept).
# This means that co-data is required for each dummy variable.
# Dat_EBcoBART does that for you and also sets X in the right format for dbarts.
# CoData: The codata model matrix with co-data information on the p covariates.
# Should be a matrix object.
# The number of rows of the co-data matrix should equal the number of columns of X.
# Again, if X contains factor variables than co-data is required for each unique
# category of the categorical variable.
# model: either continuous or binary (0,1)
# nIter: number of iterations of EM algorithm. Usually, 10 is sufficient (check WAIC trajectory)
# EB: logical, asks whether hyperparameters k and alpha (base in dbarts package) should be updated.
# defaults to FALSE (fixed k and alpha). True increases computational time.
# Prob_Init: initial covariate splitting probabilities, i.e the
# probability that covariate is considered in the splitting rules
# This parameter will be estimated
# Info: print information on fitting process
# seed: fix seed for reproducibility
# additional parameters required for fitting a BART model using dbarts. See dbarts documentation for
# meaning of these parameters. Note that our alpha and beta correspond to base and beta in dbarts, respectively
# load required R packages
if (!suppressMessages(require(dbarts, quietly = T))) {stop("Package dbarts not installed")}
if (!suppressMessages(require(loo, quietly = T))) {stop("Package loo not installed")}
# control statements
if(!(model == "continuous" | model == "binary")){stop("model should be specified as continuous or binary")}
if (class(EB) != "logical"){stop("EB is not logical, specify as either TRUE or FALSE")}
if (ncol(X) == 0 | nrow(X) == 0){stop("X not specified")}
if (ncol(CoData) == 0 | nrow(CoData) == 0){stop("CoData  not specified")}
if (length(Y) == 0){stop("Y vector is empty")}
if(!(class(Y) == "numeric")) {stop("Y is not a numeric. If Y is binary please specify it as numeric vector coded with 0 and 1")}
if (class(X)[1] != "matrix"){stop("X should be specified as matrix or a data.frame)")}
if(class(CoData)[1] != "matrix" ){stop("CoData should be specified as a matrix. Please encode dummies yourself")}
if (model=="continuous" & length(unique(Y)) < 3){stop("Y has less than 3 distinct values while model=linear is specified")}
if (model=="binary" & !all(Y==1 | Y== 0)){stop("Binary model, specify binary response as numeric coded with 0 and 1")}
if(ncol(X) != nrow(CoData)){stop("number of columns of X should equal number of rows of CoData")}
if(nchain<3){stop("Use at least 3 independent chains")}
if(!all(c(alpha,beta,k,nchain,ndpost,nskip,nIter,keepevery,ntree)>0)){stop("Check if input for bart are all positive numerics")}
# Initialization
p <- ncol(X)
probs <- Prob_Init # initial probabilities a covariate gets selected in the splitting rules
CoData <- data.frame(CoData) #required for glm fit
# storage containers
EstimatedProbs <- matrix(NA, nrow = nIter+1, ncol = ncol(X))
Codatamodels <- vector("list", length = nIter)
EstimatedProbs[1,] <- probs
row.names(EstimatedProbs) <- c(paste("Iter",0:(nIter), sep = " "))
WAICVector <- c()
WAIC_Old <- 10e8
if (EB == T){
k_Update <- c()
alpha_Update <- c()
}
for (i in 1:nIter) {
if (Info==T){
print(paste("EM iteration",i,sep = " "))
}
### step 1: Fit BART model ###
##############################
if(model == "continuous"){
if(Seed){
set.seed(4*i^2+202+3*i)
}
fit <- dbarts::bart(x.train = X, y.train = Y, # training data
ndpost = ndpost,   # number of posterior samples
nskip = nskip, # number of "warmup" samples to discard
nchain = nchain,   # number of independent, parallel chains
keepevery = keepevery, # thinning
ntree = ntree,    # number of trees per chain
keeptrees = EB,
verbose = F,
k = k, base = alpha, power = beta, # hyperparameters tree
sigest = sigest,sigdf = sigdf, sigquant = sigquant,  # hyperparameters error variance
splitprobs = probs,
combinechains = T)# hyperparameter that will be updated using EB and co-data
## MCMC Convergence check ##
if (i==1){
if (Info==T){
print("Check convergence of mcmc chains")
}
samps=fit$sigma
samps<-matrix(samps, nrow=ndpost,ncol = nchain, byrow = T)
print(dim(samps))
Rhat <- .MCMC_convergence(samps)
if (Info==T){
print(paste("Rhat equals: ",Rhat))
}
remove(samps)
if(Rhat<1.1){
if (Info==T){
print("convergence okay")
}
} else {
stop("Not converged yet, please change mcmc sampling settings")
}
}
## Estimate WAIC ##
Ypred = fit$yhat.train
LogLikMatrix = .LikelihoodCont(Ypred = Ypred, Y = Y, sigma = fit$sigma)
WAICVector[i] <- suppressWarnings(loo::waic(LogLikMatrix)$estimates[3,1])
if (Info==T){
print(paste("WAIC equals: ",WAICVector[i]))
}
}
if(model=="binary"){
if(Seed){
set.seed(4*i^2+202+3*i)
}
fit <- dbarts::bart(x.train = X, y.train = Y,
#x.test = Xtest,
ndpost = ndpost,                   # number of posterior samples
nskip = nskip,                    # number of "warmup" samples to discard
nchain = nchain,                      # number of independent, parallel chains
keepevery = keepevery, # thinning
ntree = ntree,                       # number of trees per chain
verbose = F,
usequants = F,
k = k, base = alpha, power = beta, # hyperparameters tree
splitprobs = probs,                # prob that variable is chosen for split
keeptrees = EB,             #set to True if updating alpha and k and to False if not
combinechains = T)
## MCMC Convergence check
if (i==1){
if (Info==T){
print("Check convergence of mcmc chains")
}
samps=fit$yhat.train[,sample(1:length(Y),1)]
samps<-matrix(samps, nrow=ndpost,ncol = nchain, byrow = T)
Rhat <- .MCMC_convergence(samps)
if (Info==T){
print(paste("Rhat equals: ",Rhat))
}
remove(samps)
if(Rhat<1.1){
if (Info==T){
print("convergence okay")
}
} else {
stop("Not converged yet, please change mcmc sampling settings")
}
}
## Estimate WAIC
Ypred = pnorm(fit$yhat.train)
Ypred[which(Ypred==0)] <- .0000000000000001
Ypred[which(Ypred==1)] <- .9999999999999999
LogLikMatrix = .LikelihoodBin(Ypred = Ypred, Y = Y)
WAICVector[i] <- suppressWarnings(loo::waic(LogLikMatrix)$estimates[3,1])
if (Info==T){
print(paste("WAIC equals: ",WAICVector[i]))
}
}
#### convergence check of EM algorithm
if (WAICVector[i]>WAIC_Old) {
EstProb = EstimatedProbs[i-1,]
EstWAIC = WAIC_Old
CodataModel <-  Codatamodels[[i-1]]
if (EB == T) {
Estk = k_Update[i-1]
EstAlpha = alpha_Update[i-1]
}
break
} else {
WAIC_Old <- WAICVector[i]
}
# obtain average number of times each variable occurs in the splitting rules
VarsUsed <- colSums(fit$varcount)  # count of each variable occuring in the splitting rules
VarsUsed <- VarsUsed/sum(VarsUsed) # normalize count of each variable to probabilities = pure EB updates of hyperparameter S
### STEP 2: Fit co-data model ###
coDataModel <- glm(VarsUsed ~.-1,
data=CoData,family=quasibinomial) # the model
Codatamodels[[i]] <- coDataModel
probs <- predict(coDataModel, type="response", newdata = CoData) # estimating the co-data moderated estimates of hyperparameter S
probs[is.na(probs)] <- 0
probs <-unname(probs)
EstimatedProbs[i+1,] <- probs
## Optional step: update other hyperparameters (alpha and k) of BART using Empirical Bayes ##
if (EB == T) {
trees <- dbarts::extract(fit,"trees",chainNum = c(1:nchain), sampleNum=c(sample(1:ndpost,0.25*ndpost,replace = F))) # tree structures, for computation, we only select a part of the tree
#trees <- extract(fit,"trees") # tree structures, for computation, we only select a part of the tree
# Update leaf node parameter k
k <- .EstimateLeafNode(Trees = trees, ntree = ntree, model = model)[2]
k_Update[i] <- k
# Update tree structure parameter alpha, we keep beta fixed
trees <- trees[c("tree", "sample", "chain", "n","var","value" )]
trees$depth <- unname(unlist(by(trees, trees[,c("tree", "sample", "chain")], .getDepth)))
alpha <- optim(alpha,.LikelihoodTreeStructure, beta = beta, Trees=trees, method = 'Brent', lower = .00001, upper = .9999999)$par
alpha_Update[i] <- alpha
remove(trees)
}
if (i==nIter){
print("EM algorithm not converged yet, consider increasing nIter")
print("Return estimates at last iteration")
EstProb = EstimatedProbs[i,]
EstWAIC = WAICVector[i]
CodataModel <-  Codatamodels[[i]]
if (EB == T) {
Estk = k_Update[1]
EstAlpha = alpha_Update[i]
}
}
}
# collect results
if (EB == T){
res <- list(SplittingProbs = EstProb, Codatamodel= CodataModel, k_ests = Estk,alpha_ests = EstAlpha)
} else {
res <- list(SplittingProbs = EstProb, Codatamodels= CodataModel)
}
return(res)
}
###################################
####### Auxiliary Functions #######
###################################
.FiniteSum <- function(x) {
sum(x[is.finite(x)])
}
.MCMC_convergence <- function(Samples){
if (!suppressMessages(require(posterior, quietly = T))) {stop("Package posterior not installed")}
if (class(Samples)[1] != "matrix") {stop("Samples should be specified as matrix with nsample rows and nchain columns")}
if(nrow(Samples)<ncol(Samples)){print("Are you sure Samples is specified by nsample rows and nchain columns")}
Rhat = posterior::rhat(Samples)
return(Rhat)
}
.LikelihoodBin <- function(Ypred,Y){
result <- apply(Ypred,1, function(x) Y*log(x)+(1-Y)*log(1-x))
return(t(result))
}
.LikelihoodCont <- function(Ypred, Y,sigma){
# returns the normal likelihoods for the sampled posterior parameters
# output is a N x m matrix, with N the number of samples and m the number of mc samples
loglik <- -(0.5*(1/sigma^2))*(sweep(Ypred,2,Y)^2)-.5*log(sigma^2)-.5*log(2*pi)
return(loglik)
}
.getDepth <- function(tree) {
getDepthRecurse <- function(tree, depth) {
node <- list(
depth = depth
)
if (tree$var[1] == -1) {
node$n_nodes <- 1
return(node)
}
headOfLeftBranch <- tree[-1,]
left <- getDepthRecurse(headOfLeftBranch, depth + 1)
n_nodes.left <- left$n_nodes
left$n_nodes <- NULL
headOfRightBranch <- tree[seq.int(2 + n_nodes.left, nrow(tree)),]
right <- getDepthRecurse(headOfRightBranch, depth + 1)
n_nodes.right <- right$n_nodes
right$n_nodes <- NULL
node$n_nodes <- 1 + n_nodes.left + n_nodes.right
node$depth <- c(node$depth, left$depth, right$depth)
return(node)
}
result <- getDepthRecurse(tree, 0)
return(result$depth)
}
.LikelihoodTreeStructure <- function(alpha,beta, Trees) {
LogLike <- ifelse(Trees$var==-1,log(1-alpha*(1+Trees$depth)^(-beta)),log(alpha*(1+Trees$depth)^(-beta)))
S <- .FiniteSum(LogLike)
return(-S)
}
.EstimateLeafNode <- function(Trees, ntree, model) {
if(!(model == "continuous" || model =="binary")){stop("model should be specified as continuous or binary")}
ids <- which(Trees$var==-1) #check which rows correspond to leaf nodes
samples <- Trees$value[ids] #obtain samples of leaf nodes
varhat <- (1/length(samples))*.FiniteSum(samples^2) #maximum likelihood estimate of variance for known mean (equals 0)
if (model=="continuous"){cnst = 0.5}
if (model=="binary"){cnst = 3}
k_hat <- cnst/(sqrt(varhat)*sqrt(ntree))
return(c(varhat=varhat,k_hat=k_hat))
}
p <- 500
sigma <- 1.0
N <- 100
G <- 5   #number of groups
CoDat = rep(1:G, rep(p/G,G))
CoDat = data.frame(factor(CoDat))
CoDat <- model.matrix(~0+., CoDat)
colnames(CoDat)  = paste0("Group ",1:G)
g <- function(x) {
10 * sin(pi * x[,1] * x[,2]) + 20 * (x[,101] - 0.5)^2 + 10 * x[,102] + 10 * x[,3]
}
X <- matrix(runif(N * p), N, p)
#colnames(X)<-paste0("x",seq(1,p))
Y <- g(X)+ rnorm(N, 0, sigma)
source('EBcoBART_MainFunction.R')
a=EBcoBART(Y=Y,X=X,CoData = CoDat, nIter = 15, model = "binary",
EB = T, Info = T, Seed = T,
k = 2, alpha = .95, beta = 2)
a=EBcoBART(Y=Y,X=X,CoData = CoDat, nIter = 15, model = "continuous",
EB = T, Info = T, Seed = T,
k = 2, alpha = .95, beta = 2)
a$SplittingProbs
a$k_ests
a$alpha_ests
a=EBcoBART(Y=Y,X=X,CoData = CoDat, nIter = 15, model = "continuous",
EB = F, Info = T, Seed = T,
k = 2, alpha = .95, beta = 2)
a$SplittingProbs
a=EBcoBART(Y=Y,X=X,CoData = CoDat, nIter = 15, model = "continuous",
EB = F, Info = T, Seed = T,
k = 2, alpha = .5, beta = 4)
a$SplittingProbs
a$k_ests
a$alpha_ests
